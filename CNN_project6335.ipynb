{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b94f7496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, roc_curve, roc_auc_score\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bf669ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model (data_directory,checkpoint_path, batch_size, image_width, image_height, learning_rate):\n",
    "    # Create data generators for training and validation\n",
    "    datagen = ImageDataGenerator(rescale=1./255, rotation_range=90, validation_split= 0.1)\n",
    "\n",
    "    train_generator = datagen.flow_from_directory(\n",
    "        data_directory,\n",
    "        target_size=(image_height, image_width),\n",
    "        color_mode='rgb',\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='training')\n",
    "\n",
    "    validation_generator = datagen.flow_from_directory(\n",
    "        data_directory,\n",
    "        target_size=( image_height, image_width),\n",
    "        color_mode='rgb',\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        subset='validation')\n",
    "\n",
    "    num_classes = train_generator.num_classes\n",
    "    \n",
    "    # Define and compile your CNN model\n",
    "    INPUT_SHAPE = ( image_width, image_height, 3)   #change to (SIZE, SIZE, 3)\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), 1, activation=\"relu\", input_shape=(image_width, image_height, 3)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Dropout(0.25))  # Add dropout with a rate of 25%\n",
    "\n",
    "    model.add(Conv2D(48, (3, 3), 1, activation=\"relu\"))\n",
    "    model.add(MaxPooling2D())\n",
    "    # model.add(Dropout(0.25))  # Add dropout with a rate of 25%\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), 1, activation=\"relu\"))\n",
    "    model.add(MaxPooling2D())\n",
    "    # model.add(Dropout(0.25))  # Add dropout with a rate of 25%\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(100, activation=\"relu\"))\n",
    "\n",
    "    # model.add(Dropout(0.5))  # Add dropout with a rate of 50%\n",
    "\n",
    "    model.add(Dense(num_classes,activation=\"softmax\"))  \n",
    "\n",
    "    model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate),loss='categorical_crossentropy',metrics = [\"accuracy\"])\n",
    "    print(model.summary())  \n",
    "    \n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    cp_callback = ModelCheckpoint(filepath=checkpoint_path, save_weights_only=False, verbose=1, save_best_only= True, monitor= \"val_loss\")\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(train_generator, validation_data=validation_generator, epochs=5, callbacks=[cp_callback])\n",
    "\n",
    "\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    validation_images = []\n",
    "    validation_labels = []\n",
    "    for i in range(len(validation_generator)):\n",
    "        batch_images, batch_labels = validation_generator[i]\n",
    "        validation_images.append(batch_images)\n",
    "        validation_labels.append(batch_labels)\n",
    "\n",
    "    validation_images = tf.concat(validation_images, axis=0)\n",
    "    validation_labels = tf.concat(validation_labels, axis=0)\n",
    "\n",
    "    # Perform predictions on the validation set\n",
    "    predictions = model.predict(validation_images)\n",
    "    predicted_labels = tf.squeeze(tf.round(predictions))\n",
    "\n",
    "    # Generate classification report\n",
    "\n",
    "    classification_rep = classification_report(validation_labels, predicted_labels)\n",
    "    print(classification_rep)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b73298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13169 images belonging to 3 classes.\n",
      "Found 1461 images belonging to 3 classes.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 54, 54, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 27, 27, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 27, 27, 32)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 25, 25, 48)        13872     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 12, 12, 48)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 10, 10, 32)        13856     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 5, 5, 32)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 800)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               80100     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 3)                 303       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109027 (425.89 KB)\n",
      "Trainable params: 109027 (425.89 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "2633/2634 [============================>.] - ETA: 0s - loss: 1.0992 - accuracy: 0.3443\n",
      "Epoch 1: val_loss improved from inf to 1.09973, saving model to C:\\Users\\Documents\\math_project\\model_archive\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Documents\\math_project\\model_archive/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Documents\\math_project\\model_archive/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2634/2634 [==============================] - 101s 38ms/step - loss: 1.0992 - accuracy: 0.3444 - val_loss: 1.0997 - val_accuracy: 0.3238\n",
      "Epoch 2/5\n",
      "2633/2634 [============================>.] - ETA: 0s - loss: 1.0990 - accuracy: 0.3460\n",
      "Epoch 2: val_loss improved from 1.09973 to 1.09804, saving model to C:\\Users\\Documents\\math_project\\model_archive\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Documents\\math_project\\model_archive/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Documents\\math_project\\model_archive/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2634/2634 [==============================] - 99s 38ms/step - loss: 1.0989 - accuracy: 0.3460 - val_loss: 1.0980 - val_accuracy: 0.3525\n",
      "Epoch 3/5\n",
      "2633/2634 [============================>.] - ETA: 0s - loss: 1.0988 - accuracy: 0.3478\n",
      "Epoch 3: val_loss did not improve from 1.09804\n",
      "2634/2634 [==============================] - 102s 39ms/step - loss: 1.0988 - accuracy: 0.3479 - val_loss: 1.0983 - val_accuracy: 0.3525\n",
      "Epoch 4/5\n",
      "2634/2634 [==============================] - ETA: 0s - loss: 1.0987 - accuracy: 0.3476\n",
      "Epoch 4: val_loss improved from 1.09804 to 1.09803, saving model to C:\\Users\\Documents\\math_project\\model_archive\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\Documents\\math_project\\model_archive/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Documents\\math_project\\model_archive/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2634/2634 [==============================] - 105s 40ms/step - loss: 1.0987 - accuracy: 0.3476 - val_loss: 1.0980 - val_accuracy: 0.3525\n",
      "Epoch 5/5\n",
      "2083/2634 [======================>.......] - ETA: 19s - loss: 1.0981 - accuracy: 0.3494"
     ]
    }
   ],
   "source": [
    "data_directory = 'train'\n",
    "\n",
    "checkpoint_path='C:\\\\Users\\\\Documents\\\\math_project\\\\model_archive'\n",
    "#validation_data_directory = 'C:\\\\Users\\\\Documents\\\\validation_images'\n",
    "#test_data_directory = 'C:\\\\Users\\\\Documents\\\\test_images'\n",
    "\n",
    "# Define the batch size and image dimensions\n",
    "\n",
    "batch_size = 5\n",
    "image_width, image_height = 56 , 56\n",
    "learning_rate=0.005\n",
    "train_model(data_directory,checkpoint_path, batch_size, image_width, image_height, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8e8441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
